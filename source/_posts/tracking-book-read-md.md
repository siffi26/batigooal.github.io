---
title: tracking_book_read.md
date: 2018-06-01 11:16:15
tags:
---

跟踪算法领域大神人物Martin Danelljan在5月份放出一本视觉跟踪的科普书籍《Learning Convolution Operators for Visual Tracking》，打算认真的拜读一下，特此做下翻译以及笔记，以供查阅。

# 《学习用于视觉跟踪的卷积操作》
## 摘要
本书主要目的是研究并推广*Discriminative Correlation Filter(DCF)*框架，这个框架非常适合跟踪应用。利用傅立叶变换的特点，相关滤波器通过有效地最小化最小二乘目标来学习鉴别。得到的滤波器可以在新图像上用来估计目标位置。

DCF在外观模型的学习方面的贡献包括：
+ 更新策略以及数值求解器用来解决外观模型的更新问题非常有效
+ DCF中的循环卷积所引入的周期性假设问题通过提出空间正则化部件来抵消
+ 提出了训练集的自适应模型以减轻损坏或错误标注的训练样本的影响
+ DCF所引入的一个连续空间公式使得多分辨率特征和亚像素精确预测的融合成为可能
+ 计算复杂度和过拟合问题可以通过降维技术解决

DCF的第二个贡献在于引入了用于跟踪的不同特征表达，尤其是在颜色特征的分析上。这篇论文显示了浅层和深层都是有积极作用的；另外还研究了融合它们互补性质的问题。

DCF的最后一个贡献就是考虑到了目标尺度的预测问题，提出了一维尺度滤波器，实现了高效和准确的尺度估计。

## PART 1 背景
### 1. 简介
#### 1.1 视觉跟踪
*offline learning*的意思是需要跟踪的目标类别事先知道，因此可以提前加入先验知识。*generic visual tracking*是*online*学习目标的外观。

#### 1.2 贡献
这篇文章主要研究并改进一类通用视觉跟踪方法，叫做Discriminative Correlation Filter（DCF）。最早的文献是2010年的MOSSE。本质上DCF是学习一个线性回归器来区分目标对象和周围的背景。关键思想是在整副图像上将一个线性回归器的平移不变性应用建模为一个循环关联。这使得通过利用快速傅立叶变换（FFT）来进行模型推测和预测更加有效。

基于DCF的跟踪框架大部分的贡献都是和外观模型的学习与使用相关的，包括：
+ 新的模型更新策略和优化技术使得在使用多维度图像特征时可以更鲁棒更高效的在线推理外观模型
+ 一种空间正则化组件用来减少由循环相关所引起的消极作用
+ 一种用来联合推理外观模型和训练样本权重的学习公式通过减少受损样本的影响而降低了模型的偏移
+ 一个在连续空间域中学习卷积算子的理论框架，可实现多分辨率深度特征图和亚像素定位的集成
+ 降维技术用于减少计算复杂度和过拟合
+ 研究了尺度预测问题并且引入了一种有效的一维尺度过滤方法

DCF跟踪的另外一个重要的方面是图像特征的选择和集成。综合评估了颜色特征。另外，深度卷积特征也被整合并用于基于DCF的跟踪上。最后，研究了通过将深度网络应用于光流图像计算的深度运动特征的影响。

Paper C提出的DSST跟踪器，Paper D提出的SRDCF跟踪器，Paper H提出的C-COT在本文都有提及。

#### 1.3 大纲
这篇论文第一部分包括五个章节：背景介绍、视觉跟踪理论简介以及MOSSE、DCF框架介绍、图像特征的应用介绍、总结

第二部分包括这篇论文提及到的所有文献。

#### 1.4 涉及到的文献
#### 1.5 其他出版物
文献太多，只能说太牛，需要一篇篇的精读

### 2. 从LUCAS-KANADE到MOSSE
#### 2.1 一个基础跟踪模型
我们的任务是在另外一帧图像上定位patch，首先我们构建目标的外观模型。第二步使用这个外观模型在其他图像上搜索相应的区域。作为首选，我们对感兴趣区域的像素值的联合概率密度函数建模。这样的模型叫做generative，因为他们描述了观测数据$x$。原理上，我们可以使用generative模型$p(x)$来生成目标patch的不同版本，通过从分布$p(x)$中采样。

我们考虑最简单的generative模型的建模方法。让$p(x)$满足multivariate Gaussian分布$p(x)=N(x;\mu,\sigma^2I)$，其中$\mu$是期望，$\sigma^2I$是协方差，$I$表示单位矩阵。简单期间，我们将方差当作标量超参数。均值参数$\mu$可以通过最大似然来推算：

#### 2.2 结合背景信息
现在我们看到了LK和NCC跟踪器中使用的目标外观的generative模型之间的close relationship。我们得到的结论是它们的基本模型本质上是相同的，方法仅仅不同在于求解公式2.2中的$u$所使用的优化算法不同。这些方法最大的局限是外观模型$p(x)$对于图像背景是不可知的。背景中经常包含了与目标相似的区域，这些区域通常称为"distractors"。如果我们忽视外观模型中干扰因素的建模，那我们也忽视了我们的目标与干扰因素的外观区分开来的重要细节。但是理想情况下，这些细节应该考虑进去，甚至需要被模型强调。

#### 2.3 生成方法 vs 判别方法
